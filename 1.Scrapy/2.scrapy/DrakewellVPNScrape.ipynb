{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取 CSV 数据，确保日期格式正确。\n",
    "\n",
    "按 cosit 分组，并找到每个 cosit 内的连续日期段。\n",
    "\n",
    "只保留 2023-2025 年的连续日期数据。\n",
    "\n",
    "在筛选后的数据集 subset_df 中添加 Tried 和 Retrieved 两列，初始值设为 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入库\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import requests\n",
    "import subprocess\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id    day        date         cosit\n",
      "0  573292FF-2F4B-4721-92BB-B2278A482D0F  13882  2008-01-04  000000010183\n",
      "1  573292FF-2F4B-4721-92BB-B2278A482D0F  13883  2008-01-05  000000010183\n",
      "2  573292FF-2F4B-4721-92BB-B2278A482D0F  13884  2008-01-06  000000010183\n",
      "3  573292FF-2F4B-4721-92BB-B2278A482D0F  13885  2008-01-07  000000010183\n",
      "4  573292FF-2F4B-4721-92BB-B2278A482D0F  13886  2008-01-08  000000010183\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载CSV文件和Excel文件\n",
    "ccs_cosit_date_sorted = pd.read_csv(\"ccs_cosit_date_sorted.csv\")\n",
    "\n",
    "# 加载Excel文件时明确指定 cosit 列的类型为字符串\n",
    "cosit_and_id_for_ccs_dot = pd.read_excel(\"cosit_and_id_for_ccs_dot.xlsx\", dtype={\"cosit\": str})\n",
    "\n",
    "# 根据 'id' 列进行合并，将 'cosit' 列替换到 'ccs_cosit_date_sorted' 中\n",
    "merged_data = pd.merge(ccs_cosit_date_sorted, cosit_and_id_for_ccs_dot[['id', 'cosit']], on='id', how='left')\n",
    "\n",
    "# 替换合并后的数据中的 'cosit' 列\n",
    "merged_data['cosit'] = merged_data['cosit_y']  # 用 'cosit_y' 列替换原来的 'cosit' 列\n",
    "\n",
    "# 删除 'cosit_y' 列（合并时自动生成的）\n",
    "merged_data = merged_data.drop(columns=['cosit_y'])\n",
    "merged_data = merged_data.drop(columns=['cosit_x'])\n",
    "\n",
    "# 确保 'cosit' 列保存为字符串格式\n",
    "merged_data['cosit'] = merged_data['cosit'].astype(str)\n",
    "\n",
    "# 打印合并后的数据（可选）\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改了聚合方式：按月聚合\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 读取数据\n",
    "df = merged_data\n",
    "\n",
    "# 确保日期格式正确\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 按 cosit 和年份-月份 分组\n",
    "results = []\n",
    "\n",
    "for (cosit, year_month), group in df.groupby(['cosit', df['date'].dt.to_period('M')]):\n",
    "    start_date = group['date'].min()  # 该 cosit 在该月的最早日期\n",
    "    end_date = group['date'].max()    # 该 cosit 在该月的最晚日期\n",
    "    results.append([cosit, start_date, end_date])\n",
    "\n",
    "# 生成 DataFrame\n",
    "contiguous_df = pd.DataFrame(results, columns=['cosit', 'begin_date', 'end_date'])\n",
    "\n",
    "# 过滤 2023-2025 年的数据\n",
    "subset_df = contiguous_df[\n",
    "    (contiguous_df['begin_date'].dt.year >= 2023) & (contiguous_df['begin_date'].dt.year <= 2025) & \n",
    "    (contiguous_df['end_date'].dt.year >= 2023) & (contiguous_df['end_date'].dt.year <= 2025)\n",
    "].copy()\n",
    "\n",
    "# 添加标记列\n",
    "subset_df.loc[:, 'Tried'] = 0\n",
    "subset_df.loc[:, 'Retrieved'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.to_excel(\"subset_ccs_cosit_date_sorted_by_month_exact.xlsx\", index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从这里开始跑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过已下载的文件名更新下载记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新后的文件已保存到: ./updated_excel.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 读取主Excel文件\n",
    "excel_file = \"subset_ccs_cosit_date_sorted_by_month_exact_feb261013am.xlsx\"\n",
    "df_main = pd.read_excel(excel_file, dtype={\"cosit\": str})\n",
    "\n",
    "# 指定 NamedReports 文件夹路径\n",
    "xls_folder = \"./NamedReports/\"\n",
    "\n",
    "# 获取所有 xls 文件名\n",
    "xls_files = [f for f in os.listdir(xls_folder) if f.endswith(\".xls\")]\n",
    "\n",
    "# 提取 cosit、begin_date 和 end_date\n",
    "data_records = []\n",
    "pattern = r\"(\\d+)_([\\d-]+)_([\\d-]+)\\.xls\"\n",
    "\n",
    "for filename in xls_files:\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        cosit = match.group(1)\n",
    "        begin_date = match.group(2)\n",
    "        end_date = match.group(3)\n",
    "        data_records.append({\"cosit\": cosit, \"begin_date\": begin_date, \"end_date\": end_date})\n",
    "\n",
    "# 创建 DataFrame\n",
    "df_downloaded = pd.DataFrame(data_records)\n",
    "\n",
    "# 确保主 Excel 的数据格式一致\n",
    "df_main[\"begin_date\"] = df_main[\"begin_date\"].astype(str)\n",
    "df_main[\"end_date\"] = df_main[\"end_date\"].astype(str)\n",
    "df_main[\"cosit\"] = df_main[\"cosit\"].astype(str)\n",
    "\n",
    "# 检查 \"Retrieved\" 列是否存在\n",
    "if \"Retrieved\" not in df_main.columns:\n",
    "    raise ValueError(\"主 Excel 文件中不存在 'Retrieved' 列，请检查数据格式！\")\n",
    "\n",
    "# 更新 \"Retrieved\" 列\n",
    "df_main[\"Retrieved\"] = df_main.apply(\n",
    "    lambda row: 1 if ((df_downloaded[\"cosit\"] == row[\"cosit\"]) &\n",
    "                      (df_downloaded[\"begin_date\"] == row[\"begin_date\"]) &\n",
    "                      (df_downloaded[\"end_date\"] == row[\"end_date\"])).any() else row[\"Retrieved\"], axis=1\n",
    ")\n",
    "\n",
    "# 保存更新后的 Excel\n",
    "updated_file = \"./updated_excel.xlsx\"\n",
    "df_main.to_excel(updated_file, index=False)\n",
    "\n",
    "print(f\"更新后的文件已保存到: {updated_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Failed Data to Excel, Rescrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取数据\n",
    "updated_excel = pd.read_excel(\"updated_excel.xlsx\", dtype={'cosit': str})\n",
    "processing_log = pd.read_excel(\"processing_log.xlsx\")\n",
    "\n",
    "# 解析 file_name，提取 cosit, begin_date, end_date\n",
    "def extract_info(file_name):\n",
    "    match = re.match(r\"(\\d+)_(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})\\.xls\", file_name)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    return None, None, None\n",
    "\n",
    "# 解析 processing_log 的 cosit, begin_date, end_date\n",
    "processing_log[['cosit', 'begin_date', 'end_date']] = processing_log['File_Name'].apply(lambda x: pd.Series(extract_info(x)))\n",
    "\n",
    "# 确保 cosit 统一为字符串类型\n",
    "processing_log['cosit'] = processing_log['cosit'].astype(str)\n",
    "processing_log['begin_date'] = processing_log['begin_date'].astype(str)\n",
    "processing_log['end_date'] = processing_log['end_date'].astype(str)\n",
    "\n",
    "# 仅保留 `Status` 为 `Failed` 的记录\n",
    "failed_logs = processing_log[processing_log['Status'] == 'Failed'][['cosit', 'begin_date', 'end_date']]\n",
    "\n",
    "# 确保 updated_excel 的 cosit 也是字符串类型\n",
    "updated_excel['cosit'] = updated_excel['cosit'].astype(str)\n",
    "updated_excel['begin_date'] = updated_excel['begin_date'].astype(str)\n",
    "updated_excel['end_date'] = updated_excel['end_date'].astype(str)\n",
    "\n",
    "# 合并数据\n",
    "merged = updated_excel.merge(failed_logs, on=['cosit', 'begin_date', 'end_date'], how='left', indicator=True)\n",
    "\n",
    "# 将 `_merge` 列值为 'both' 的行的 `Tried` 和 `Retrieved` 设为 0\n",
    "merged.loc[merged['_merge'] == 'both', ['Tried', 'Retrieved']] = 0\n",
    "\n",
    "# 删除 `_merge` 列\n",
    "merged.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "# 保存更新后的 `updated_excel`\n",
    "merged.to_excel(\"updated_excel_add_failed_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新数据更新文件\n",
    "主要修改点：\n",
    "仅更新 Retrieved 和 Tried 列：\n",
    "\n",
    "之前的代码只更新 Retrieved，现在 Tried 也同步更新。\n",
    "只有当文件存在时，Retrieved 和 Tried 才设为 1，否则保持原值。\n",
    "检查 Retrieved 和 Tried 是否存在：\n",
    "\n",
    "如果这两列在 Excel 中不存在，则创建，并初始化为 0。\n",
    "使用 apply 更新数据：\n",
    "\n",
    "update_flags() 函数检查 df_downloaded 是否包含对应的 cosit、begin_date 和 end_date 组合，如果存在，则 Retrieved=1, Tried=1，否则保持原值。\n",
    "运行结果：\n",
    "代码执行后，updated_excel.xlsx 只会更新那些 成功下载的文件对应的行，不会影响其他数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "更新后的文件已保存到: ./updated_excel.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 读取主Excel文件\n",
    "excel_file = \"updated_excel.xlsx\"\n",
    "df_main = pd.read_excel(excel_file, dtype={\"cosit\": str})\n",
    "\n",
    "# 指定 NamedReports 文件夹路径\n",
    "xls_folder = \"./NamedReports/\"\n",
    "\n",
    "# 获取所有 xls 文件名\n",
    "xls_files = [f for f in os.listdir(xls_folder) if f.endswith(\".xls\")]\n",
    "\n",
    "# 提取 cosit、begin_date 和 end_date\n",
    "data_records = []\n",
    "pattern = r\"(\\d+)_([\\d-]+)_([\\d-]+)\\.xls\"\n",
    "\n",
    "for filename in xls_files:\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        cosit = match.group(1)\n",
    "        begin_date = match.group(2)\n",
    "        end_date = match.group(3)\n",
    "        data_records.append({\"cosit\": cosit, \"begin_date\": begin_date, \"end_date\": end_date})\n",
    "\n",
    "# 创建 DataFrame\n",
    "df_downloaded = pd.DataFrame(data_records)\n",
    "\n",
    "# 确保主 Excel 的数据格式一致\n",
    "df_main[\"begin_date\"] = df_main[\"begin_date\"].astype(str)\n",
    "df_main[\"end_date\"] = df_main[\"end_date\"].astype(str)\n",
    "df_main[\"cosit\"] = df_main[\"cosit\"].astype(str)\n",
    "\n",
    "# 确保 \"Retrieved\" 和 \"Tried\" 列存在，否则创建\n",
    "for col in [\"Retrieved\", \"Tried\"]:\n",
    "    if col not in df_main.columns:\n",
    "        df_main[col] = 0  # 如果列不存在，则创建并初始化为 0\n",
    "\n",
    "# 更新 \"Retrieved\" 和 \"Tried\" 列（仅对已下载的文件更新，不影响其他数据）\n",
    "def update_flags(row):\n",
    "    if ((df_downloaded[\"cosit\"] == row[\"cosit\"]) &\n",
    "        (df_downloaded[\"begin_date\"] == row[\"begin_date\"]) &\n",
    "        (df_downloaded[\"end_date\"] == row[\"end_date\"])).any():\n",
    "        return pd.Series({\"Retrieved\": 1, \"Tried\": 1})\n",
    "    return pd.Series({\"Retrieved\": row[\"Retrieved\"], \"Tried\": row[\"Tried\"]})\n",
    "\n",
    "# 应用更新逻辑\n",
    "df_main[[\"Retrieved\", \"Tried\"]] = df_main.apply(update_flags, axis=1)\n",
    "\n",
    "# 保存更新后的 Excel\n",
    "updated_file = \"./updated_excel.xlsx\"\n",
    "df_main.to_excel(updated_file, index=False)\n",
    "\n",
    "print(f\"更新后的文件已保存到: {updated_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cosit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "begin_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "end_date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Tried",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Retrieved",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2df836aa-a09c-4bdd-910c-8524489e01d7",
       "rows": [
        [
         "0",
         "000000010183",
         "2023-01-01 00:00:00",
         "2023-01-31 00:00:00",
         "0",
         "0"
        ],
        [
         "1",
         "000000010183",
         "2023-02-01 00:00:00",
         "2023-02-28 00:00:00",
         "0",
         "0"
        ],
        [
         "2",
         "000000010183",
         "2023-03-01 00:00:00",
         "2023-03-31 00:00:00",
         "0",
         "0"
        ],
        [
         "3",
         "000000010183",
         "2023-04-01 00:00:00",
         "2023-04-30 00:00:00",
         "0",
         "0"
        ],
        [
         "4",
         "000000010183",
         "2023-05-01 00:00:00",
         "2023-05-31 00:00:00",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosit</th>\n",
       "      <th>begin_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>Tried</th>\n",
       "      <th>Retrieved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000010183</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000010183</td>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000010183</td>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000010183</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000010183</td>\n",
       "      <td>2023-05-01</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cosit begin_date   end_date  Tried  Retrieved\n",
       "0  000000010183 2023-01-01 2023-01-31      0          0\n",
       "1  000000010183 2023-02-01 2023-02-28      0          0\n",
       "2  000000010183 2023-03-01 2023-03-31      0          0\n",
       "3  000000010183 2023-04-01 2023-04-30      0          0\n",
       "4  000000010183 2023-05-01 2023-05-31      0          0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#引入库\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import requests\n",
    "import subprocess\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# 加载Excel文件时明确指定 cosit 列的类型为字符串\n",
    "subset_df = pd.read_excel(\"./updated_excel.xlsx\", dtype={\"cosit\": str})\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df[\"begin_date\"] = pd.to_datetime(subset_df[\"begin_date\"], errors='coerce', format='%Y-%m-%d')\n",
    "subset_df[\"end_date\"] = pd.to_datetime(subset_df[\"end_date\"], errors='coerce', format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 Mullvad VPN 服务器列表\n",
    "# 该函数从 mullvad relay list 命令中提取位于指定国家的 VPN 服务器，并将其解析为 pandas.DataFrame 结构。\n",
    "# 增加了几个国家\n",
    "def get_mullvad_servers():\n",
    "    \"\"\"Fetch and parse Mullvad VPN servers in specified countries into a DataFrame.\"\"\"\n",
    "    try:\n",
    "        # Define the countries to include\n",
    "        countries = ['us', 'ca', 'gb', 'de', 'jp']\n",
    "\n",
    "        # Create a regex pattern for the selected countries\n",
    "        country_pattern = '|'.join(countries)\n",
    "\n",
    "        # Run the shell command to get valid servers\n",
    "        command = f\"mullvad relay list | awk '{{print $1}}' | grep -E '^({country_pattern})-[a-z0-9]+-[a-z0-9]+-[a-z0-9]+$'\"\n",
    "        result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            print(\"Error fetching Mullvad relay list. Ensure Mullvad VPN is installed and running.\")\n",
    "            return pd.DataFrame(columns=[\"Country\", \"City Code\"])\n",
    "\n",
    "        # Process output into a list of (country, city) tuples\n",
    "        servers = [line.strip() for line in result.stdout.split(\"\\n\") if line.strip()]\n",
    "        parsed_servers = [(s.split(\"-\")[0], s.split(\"-\")[1]) for s in servers]\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(parsed_servers, columns=[\"Country\", \"City Code\"])\n",
    "\n",
    "        # Deduplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving Mullvad server list: {e}\")\n",
    "        return pd.DataFrame(columns=[\"Country\", \"City Code\"])\n",
    "\n",
    "# 获取当前外网 IP, 获取当前设备的 外部 IP 地址。\n",
    "def get_external_ip():\n",
    "    \"\"\"Gets the current external IP address using a system command.\"\"\"\n",
    "    try:\n",
    "        #result = subprocess.run([\"curl\", \"-s\", \"ifconfig.me\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        result = subprocess.run([\"curl\", \"-s\", \"https://api.ipify.org\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.strip()\n",
    "        else:\n",
    "            logging.error(f\"Failed to retrieve external IP: {result.stderr}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error while getting external IP: {e}\")\n",
    "        return None\n",
    "\n",
    "# 切换到随机 Mullvad VPN 服务器，并确保 IP 变化\n",
    "'''\n",
    "随机选择一个服务器\n",
    "切换 VPN 服务器\n",
    "检查外网 IP 变化\n",
    "最多尝试 10 次\n",
    "'''\n",
    "def change_mullvad_server(df_servers):\n",
    "    \"\"\"Connects Mullvad VPN to a random server and ensures a different IP.\"\"\"\n",
    "    if df_servers.empty:\n",
    "        logging.error(\"No servers available to connect.\")\n",
    "        return\n",
    "\n",
    "    old_ip = get_external_ip()\n",
    "    if old_ip:\n",
    "        logging.info(f\"Current IP: {old_ip}\")\n",
    "    else:\n",
    "        logging.warning(\"Could not retrieve current IP. Proceeding anyway.\")\n",
    "\n",
    "    attempts = 0\n",
    "    new_ip = old_ip  # Initialize with old IP to enter the loop\n",
    "\n",
    "    while new_ip == old_ip and attempts < 10:\n",
    "        # Randomly select a server\n",
    "        selected_server = df_servers.sample(n=1).iloc[0]\n",
    "        country = selected_server[\"Country\"]\n",
    "        city = selected_server[\"City Code\"]\n",
    "\n",
    "        logging.info(\"Disconnecting Mullvad VPN...\")\n",
    "        subprocess.run([\"mullvad\", \"disconnect\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        time.sleep(1)\n",
    "\n",
    "        logging.info(f\"Connecting to Mullvad VPN at {city} ({country})...\")\n",
    "        subprocess.run([\"mullvad\", \"relay\", \"set\", \"location\", country, city], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        subprocess.run([\"mullvad\", \"connect\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        # Confirm VPN connection\n",
    "        status = subprocess.run([\"mullvad\", \"status\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        logging.info(f\"VPN Status: {status.stdout.strip()}\")\n",
    "\n",
    "        # Check new external IP\n",
    "        time.sleep(3)  # Wait for the connection to stabilize\n",
    "        new_ip = get_external_ip()\n",
    "\n",
    "        if new_ip and new_ip != old_ip:\n",
    "            logging.info(f\"Successfully changed IP: {new_ip}\")\n",
    "            return\n",
    "        else:\n",
    "            logging.warning(f\"Attempt {attempts + 1}: IP did not change. Retrying...\")\n",
    "\n",
    "        attempts += 1\n",
    "\n",
    "    logging.error(\"Max attempts reached. Unable to change IP.\")\n",
    "\n",
    "# 断开 Mullvad VPN\n",
    "def disconnect_mullvad():\n",
    "    \"\"\"Disconnects Mullvad VPN.\"\"\"\n",
    "    logging.info(\"Disconnecting Mullvad VPN...\")\n",
    "    subprocess.run([\"mullvad\", \"disconnect\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "配置 Selenium WebDriver\n",
    "\n",
    "初始化 Selenium WebDriver\n",
    "配置浏览器下载路径\n",
    "设定浏览器选项，减少被检测为自动化程序的可能性\n",
    "'''\n",
    "def setup_selenium_driver(download_dir):\n",
    "    \"\"\"Initializes Selenium WebDriver with correct download settings.\"\"\"\n",
    "    download_dir = os.path.abspath(download_dir)\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    #options.add_argument(\"--start-maximized\")\n",
    "\n",
    "    prefs = {\n",
    "        \"download.default_directory\": download_dir,\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"download.directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True,\n",
    "        \"safebrowsing.disable_download_protection\": True\n",
    "    }\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    logging.info(f\"Selenium WebDriver launched with download directory: {download_dir}\")\n",
    "    return driver\n",
    "\n",
    "# 下载和重命名报告\n",
    "'''\n",
    "使用 Selenium 访问指定网站\n",
    "下载 Excel 文件\n",
    "检测下载是否完成\n",
    "重命名文件并移动到 NamedReports 目录\n",
    "'''\n",
    "def download_and_rename_reports(batch_rows):\n",
    "    \"\"\"Sequentially downloads reports, renames, and moves them to NamedReports.\"\"\"\n",
    "    download_dir = os.path.abspath(\"Reports\")\n",
    "    named_reports_dir = os.path.abspath(\"NamedReports\")\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    os.makedirs(named_reports_dir, exist_ok=True)\n",
    "\n",
    "    driver = setup_selenium_driver(download_dir)\n",
    "\n",
    "    for _, row in batch_rows.iterrows():\n",
    "        cosit = row['cosit']\n",
    "        begin_date = row['begin_date'].strftime('%Y-%m-%d')\n",
    "        end_date = row['end_date'].strftime('%Y-%m-%d')\n",
    "\n",
    "        url = f\"https://gdottrafficdata.drakewell.com/tfdaysreport.asp?node=GDOT_CCS&cosit={cosit}&reportdate={begin_date}&enddate={end_date}&intval=4&dir=%-4&excel=1\"\n",
    "        logging.info(f\"Opening URL: {url}\")\n",
    "        driver.get(url)\n",
    "\n",
    "        start_time = time.time()\n",
    "        downloaded_file = None\n",
    "\n",
    "        while True:\n",
    "            xls_files = glob.glob(os.path.join(download_dir, \"*.xls\"))\n",
    "            if xls_files:\n",
    "                downloaded_file = max(xls_files, key=os.path.getctime)\n",
    "                break\n",
    "            if time.time() - start_time > 30:\n",
    "                logging.warning(f\"Timeout reached for {url}. Skipping to next.\")\n",
    "                # 增加等待时间\n",
    "                # time.sleep(600)  # Pause for 10 minute\n",
    "                break\n",
    "            time.sleep(1)\n",
    "\n",
    "        if downloaded_file:\n",
    "            new_filename = f\"{cosit}_{begin_date}_{end_date}.xls\"\n",
    "            shutil.move(downloaded_file, os.path.join(named_reports_dir, new_filename))\n",
    "            logging.info(f\"Downloaded and renamed: {new_filename}\")\n",
    "\n",
    "    driver.quit()\n",
    "    logging.info(\"Driver closed.\")\n",
    "\n",
    "# 批量爬取数据并切换 VPN\n",
    "'''\n",
    "检查 NamedReports 是否已有文件\n",
    "筛选要爬取的 batch_number 条数据\n",
    "使用 Mullvad VPN 切换 IP\n",
    "调用 download_and_rename_reports() 进行下载\n",
    "更新爬取状态\n",
    "断开 VPN\n",
    "'''\n",
    "def scrape_batch_with_vpn(batch_number):\n",
    "    \"\"\"Scrapes a batch of data, checking NamedReports first, then proceeding with the scraping process.\"\"\"\n",
    "    global subset_df\n",
    "\n",
    "    named_reports_dir = os.path.abspath(\"NamedReports\")\n",
    "\n",
    "    # Step 1: Check NamedReports and update subset_df\n",
    "    logging.info(\"Checking NamedReports for already downloaded files...\")\n",
    "\n",
    "    for idx, row in subset_df.iterrows():\n",
    "        expected_filename = f\"{row['cosit']}_{row['begin_date'].strftime('%Y-%m-%d')}_{row['end_date'].strftime('%Y-%m-%d')}.xls\"\n",
    "        if os.path.exists(os.path.join(named_reports_dir, expected_filename)):\n",
    "            subset_df.loc[idx, ['Tried', 'Retrieved']] = 1  # Mark as already scraped\n",
    "\n",
    "    logging.info(\"Finished checking NamedReports. Now selecting batch for scraping.\")\n",
    "\n",
    "    # Step 2: Main scraping loop\n",
    "    while True:\n",
    "        # Select batch where Tried = 0 first; if none, select where Tried = 1 and Retrieved = 0\n",
    "        available_rows = subset_df[subset_df['Tried'] == 0]\n",
    "        if available_rows.empty:\n",
    "            available_rows = subset_df[(subset_df['Tried'] == 1) & (subset_df['Retrieved'] == 0)]\n",
    "        \n",
    "        if available_rows.empty:\n",
    "            logging.info(\"No more files left to try. Terminating loop.\")\n",
    "            break\n",
    "\n",
    "        # Randomly select batch_number rows\n",
    "        batch_rows = available_rows.sample(n=min(batch_number, len(available_rows)), random_state=42)\n",
    "\n",
    "        # Mark selected rows as Tried\n",
    "        subset_df.loc[batch_rows.index, 'Tried'] = 1\n",
    "\n",
    "        # Change VPN to a new random location\n",
    "        change_mullvad_server(get_mullvad_servers())\n",
    "\n",
    "        # Download reports\n",
    "        download_and_rename_reports(batch_rows)\n",
    "\n",
    "        # Step 3: Check downloaded files and update Retrieved column\n",
    "        for idx, row in batch_rows.iterrows():\n",
    "            expected_filename = f\"{row['cosit']}_{row['begin_date'].strftime('%Y-%m-%d')}_{row['end_date'].strftime('%Y-%m-%d')}.xls\"\n",
    "            if os.path.exists(os.path.join(named_reports_dir, expected_filename)):\n",
    "                subset_df.loc[idx, 'Retrieved'] = 1\n",
    "\n",
    "        # Disconnect VPN\n",
    "        disconnect_mullvad()\n",
    "\n",
    "        # Pause for a random time between 1 and 2 seconds\n",
    "        # 避免vpn节点耗尽 否则10分钟耗尽一次\n",
    "        # time.sleep(random.uniform(10, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_batch_with_vpn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.to_excel(\"updated_excel.xlsx\", index=False, engine='openpyxl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
